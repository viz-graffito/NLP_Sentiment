{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://insights.blackcoffer.com/how-is-login-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://insights.blackcoffer.com/how-does-ai-h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-its-im...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://insights.blackcoffer.com/how-do-deep-l...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://insights.blackcoffer.com/how-artificia...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL  POSITIVE SCORE  \\\n",
       "0       1  https://insights.blackcoffer.com/how-is-login-...             NaN   \n",
       "1       2  https://insights.blackcoffer.com/how-does-ai-h...             NaN   \n",
       "2       3  https://insights.blackcoffer.com/ai-and-its-im...             NaN   \n",
       "3       4  https://insights.blackcoffer.com/how-do-deep-l...             NaN   \n",
       "4       5  https://insights.blackcoffer.com/how-artificia...             NaN   \n",
       "\n",
       "   NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
       "0             NaN             NaN                 NaN                  NaN   \n",
       "1             NaN             NaN                 NaN                  NaN   \n",
       "2             NaN             NaN                 NaN                  NaN   \n",
       "3             NaN             NaN                 NaN                  NaN   \n",
       "4             NaN             NaN                 NaN                  NaN   \n",
       "\n",
       "   PERCENTAGE OF COMPLEX WORDS  FOG INDEX  AVG NUMBER OF WORDS PER SENTENCE  \\\n",
       "0                          NaN        NaN                               NaN   \n",
       "1                          NaN        NaN                               NaN   \n",
       "2                          NaN        NaN                               NaN   \n",
       "3                          NaN        NaN                               NaN   \n",
       "4                          NaN        NaN                               NaN   \n",
       "\n",
       "   COMPLEX WORD COUNT  WORD COUNT  SYLLABLE PER WORD  PERSONAL PRONOUNS  \\\n",
       "0                 NaN         NaN                NaN                NaN   \n",
       "1                 NaN         NaN                NaN                NaN   \n",
       "2                 NaN         NaN                NaN                NaN   \n",
       "3                 NaN         NaN                NaN                NaN   \n",
       "4                 NaN         NaN                NaN                NaN   \n",
       "\n",
       "   AVG WORD LENGTH  \n",
       "0              NaN  \n",
       "1              NaN  \n",
       "2              NaN  \n",
       "3              NaN  \n",
       "4              NaN  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('Output.xlsx')\n",
    "df['URL_ID'] = df['URL_ID'].astype('int')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    https://insights.blackcoffer.com/how-is-login-...\n",
       "1    https://insights.blackcoffer.com/how-does-ai-h...\n",
       "2    https://insights.blackcoffer.com/ai-and-its-im...\n",
       "3    https://insights.blackcoffer.com/how-do-deep-l...\n",
       "4    https://insights.blackcoffer.com/how-artificia...\n",
       "Name: URL, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = df['URL']\n",
    "urls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some vendors (fruit and vegetable sellers) began venturing out after a few days without explicit permission and immediately faced police harassment. After a few weeks, the government eased restrictions and essential vendors were being permitted to vend (due in large part to the advocacy of vendor organizations and activist networks). However, the cost of doing business, as well as the risk, has gone up significantly, with vendors not having access to wholesale markets and suppliers and having to spend more on travel costs due to travel restrictions in place in the city. Also, with the lockdown still partially in place, the number of buyers has gone down and so have earnings. Due to the harsh summer heat, perishable fruits and vegetables also have a reduced shelf life so vendors are unable to capitalize on whatever produce they do have.\n",
      "The state has recently announced a stimulus package of INR 5000 crore for nearly 50 lakh vendors, acknowledging the grave impact of their loss of livelihood. The intended relief for vendors will be a credit loan that will provide an initial working capital of INR 10,000 for all vendors, but this is not sufficient. Instead of credit, the government should think of converting it into direct income benefit, a cash grant, as livelihood support to start the income activity regularly. The vendors need income support to be able to restart work, and if they are not able to do so, how will they return the loan. In the face of the ever-changing crisis, vendor organizations have to step forward and advocate for vendors to be provided the resources they need to be able to resume their livelihoods. To this end, vendors organizations could consider the following for an advocacy agenda:\n",
      "Livelihood promotion for all vendors, including those selling non-essential goods: The impact of COVID-19 has been very harsh on informal workers who have exhausted their capital and earnings in trying to feed themselves during the extended lockdown period. Vendors need to be able to resume vending for survival and the government should take steps to begin to reopen markets and allow vendors back on the streets.Reopening of Markets keeping in mind social distancing and hygiene: Delhi has many different types of traditionally crowded markets including weekly markets (for fresh food, cooked food and essential household items) and daily markets that operate on the sides of roads. These markets will need to resume keeping in mind the need for social distancing and the government should release guidelines for the same. Going forward, vending zones must also be designed keeping in mind the need for social distancing and for sufficient hygiene facilities (running water, washing stations and toilets). The authorities should work with Town Vending Committees (TVCs) for the same.Provide direct support which is de-linked from existing registration requirements: As lockdown is lifted and vending resumes, vendors who have been at home for months will need direct income benefits to resume their work. The government stimulus package, while a welcome step, is insufficient in the nature of relief (credit not direct cash transfer) and eligibility (only registered vendors are eligible, which leaves out the majority of vendors in the country). In addition, government relief and support needs to be de-linked from very rigid registration requirements, as very few vendors have been registered in India. In Delhi, out of roughly 300,000 street vendors, only about 131,00 have some form of occupational identification. If the criteria for any kind of cash grant or livelihood support is linked to occupational identification by the state, then the government should also accept registration with a workers’ organisation/union as a proxy for government-issued vending passes.Ensuring hygiene and social distancing at sites of vending: The government needs to take steps for provision of running water and soap/sanitisers for street vendors at their place of work. Additionally, vendor organisations should work with food safety authorities in the country to train vendors (especially cooked food vendors) in ways to maintain hygiene while working.Taking steps to survey and register more vendors for access to government benefits: As mentioned earlier, the number of vendors who have some form of identification are a fraction of the actual population of vendors in Delhi. Before the crisis and subsequent lockdown, the Town Vending Committees (TVCs) were supposed to start surveying and registering vendors. As we get used to the new normal, the process of survey and registration should also begin to ensure that all vendors are able to access social security benefits and financial aid during this period of crisis.\n",
      "Livelihood promotion for all vendors, including those selling non-essential goods: The impact of COVID-19 has been very harsh on informal workers who have exhausted their capital and earnings in trying to feed themselves during the extended lockdown period. Vendors need to be able to resume vending for survival and the government should take steps to begin to reopen markets and allow vendors back on the streets.\n",
      "Reopening of Markets keeping in mind social distancing and hygiene: Delhi has many different types of traditionally crowded markets including weekly markets (for fresh food, cooked food and essential household items) and daily markets that operate on the sides of roads. These markets will need to resume keeping in mind the need for social distancing and the government should release guidelines for the same. Going forward, vending zones must also be designed keeping in mind the need for social distancing and for sufficient hygiene facilities (running water, washing stations and toilets). The authorities should work with Town Vending Committees (TVCs) for the same.\n",
      "Provide direct support which is de-linked from existing registration requirements: As lockdown is lifted and vending resumes, vendors who have been at home for months will need direct income benefits to resume their work. The government stimulus package, while a welcome step, is insufficient in the nature of relief (credit not direct cash transfer) and eligibility (only registered vendors are eligible, which leaves out the majority of vendors in the country). In addition, government relief and support needs to be de-linked from very rigid registration requirements, as very few vendors have been registered in India. In Delhi, out of roughly 300,000 street vendors, only about 131,00 have some form of occupational identification. If the criteria for any kind of cash grant or livelihood support is linked to occupational identification by the state, then the government should also accept registration with a workers’ organisation/union as a proxy for government-issued vending passes.\n",
      "Ensuring hygiene and social distancing at sites of vending: The government needs to take steps for provision of running water and soap/sanitisers for street vendors at their place of work. Additionally, vendor organisations should work with food safety authorities in the country to train vendors (especially cooked food vendors) in ways to maintain hygiene while working.\n",
      "Taking steps to survey and register more vendors for access to government benefits: As mentioned earlier, the number of vendors who have some form of identification are a fraction of the actual population of vendors in Delhi. Before the crisis and subsequent lockdown, the Town Vending Committees (TVCs) were supposed to start surveying and registering vendors. As we get used to the new normal, the process of survey and registration should also begin to ensure that all vendors are able to access social security benefits and financial aid during this period of crisis.\n"
     ]
    }
   ],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36'}\n",
    "webpage_response = requests.get('https://insights.blackcoffer.com/impacts-of-covid-19-on-vegetable-vendors-and-food-stalls/', headers=headers)\n",
    "webpage = webpage_response.content\n",
    "soup = BeautifulSoup(webpage, \"html.parser\")\n",
    "heading = soup.find(\"h1\")\n",
    "head = heading.get_text()\n",
    "content = soup.find(attrs={'class':'td-post-content'})\n",
    "child = content.findChildren()[:-1]\n",
    "output = \"\\n\".join(x.text for x in child)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36'}\n",
    "article = []\n",
    "for url in urls:\n",
    "    webpage_response = requests.get(url, headers=headers)\n",
    "    webpage = webpage_response.content\n",
    "    soup = BeautifulSoup(webpage, \"html.parser\")\n",
    "    heading = soup.find(\"h1\")\n",
    "    head = heading.get_text()\n",
    "    content = soup.find(attrs={'class':'td-post-content'})\n",
    "    child = content.findChildren()[:-1]\n",
    "    body = \"\\n\".join(x.text for x in child)\n",
    "    article.append(str(head) + str(body))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How is Login Logout Time Tracking for Employees in Office done by AI?When people hear AI they often think about sentient robots and magic boxes. AI today is much more mundane and simple—but that doesn’t mean it’s not powerful. Another misconception is that high-profile research projects can be applied directly to any business situation. AI done right can create an extreme return on investments (ROIs)—for instance through automation or precise prediction. But it does take thought, time, and proper implementation. We have seen that success and value generated by AI projects are increased when there is a grounded understanding and expectation of what the technology can deliver from the C-suite down.\n",
      "“Artificial Intelligence (AI) is a science and a set of computational technologies that are inspired by—but typically operate quite differently from—the ways people use their nervous systems and bodies to sense, learn, reason and take action.”3 Lately there has been a big rise in the day-to-day use of machines powered by AI. These machines are wired using cross-disciplinary approaches based on mathematics, computer science, statistics, psychology, and more.4 Virtual assistants are becoming more common, most of the web shops predict your purchases, many companies make use of chatbots in their customer service and many companies use algorithms to detect fraud.\n",
      "AI and Deep Learning technology employed in office entry systems will bring proper time tracking of each employee. As this system tries to learn each person with an image processing technology whose data is feed forwarded to a deep learning model where Deep learning isn’t an algorithm per se, but rather a family of algorithms that implements deep networks (many layers). These networks are so deep that new methods of computation, such as graphics processing units (GPUs), are required to train them, in addition to clusters of compute nodes. So using deep learning we can take detect the employee using face and person recognition scan and through which login, logout timing is recorded. Using an AI system we can even identify each employee’s entry time, their working hours, non-working hours by tracking the movement of an employee in the office so that system can predict and report HR for the salary for each employee based on their working hours. Our system can take feed from CCTV to track movements of employees and this system is capable of recognizing a person even he/she is being masked as in this pandemic situation by taking their iris scan. With this system installed inside the office, the following are some of the benefits:\n",
      "1)Compliance/litigation needs\n",
      "\n",
      "1)Compliance/litigation needs\n",
      "For several countries, regulations insist that the employer must keep documents available that can demonstrate the working hours performed by each employee. In the event of control from the labor inspectorate or a dispute with an employee, the employer must be able to explain and justify the working hours for the company. This can be made easy as our system is tracking employee movements\n",
      "2)Information security needs\n",
      "2)Information security needs\n",
      "This is about monitoring user connection times to detect suspicious access times. In the event where compromised credentials are used to log on at 3 a.m. on a Saturday, a notification on this access could alert the IT team that an attack is possibly underway.\n",
      "3)Employee login logout software\n",
      "3)\n",
      "Employee login logout software\n",
      "To manage and react to employees’ attendance, overtime thresholds, productivity, and suspicious access times, our system records and stores detailed and interactive reporting on users’ connection times. These records allow you to better manage users’ connection times and provide accurate, detailed data required by management.\n",
      "4)If you want to avoid paying overtime, make sure that your employees respect certain working time quotas or even avoid suspicious access. Our system will alert the HR officer about each employee’s office in and out time so that they can accordingly take action.\n",
      "5)Last but not least it reduces human resource needs to keep track of the records and sending the report to HR and HR officials has to check through the report so this system will reduce times and human resource needs\n",
      "With the use of AI and Deep Learning technologies, we can automate some routines stuff with more functionality which humans need more resources to keep track thereby reducing time spent on manual data entry works rather companies can think of making their position high in the competitive world.\n"
     ]
    }
   ],
   "source": [
    "print(article[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_id = df['URL_ID']\n",
    "# for i in range(len(article)):\n",
    "#     fh = open('{}.txt'.format(url_id[i]), 'w', encoding=\"utf-8\")\n",
    "#     fh.write(article[i])\n",
    "#     fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Envy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Envy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk, re\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "path = './stopwords/'\n",
    "with open(path+ 'StopWords_Generic.txt', 'r+', encoding='utf-8') as file:\n",
    "    stop_words = file.readlines()\n",
    "\n",
    "\n",
    "stop_words = set(stop_words)\n",
    "\n",
    "normalizer = WordNetLemmatizer()\n",
    "\n",
    "def get_part_of_speech(word):\n",
    "  probable_part_of_speech = wordnet.synsets(word)\n",
    "  pos_counts = Counter()\n",
    "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "  return most_likely_part_of_speech\n",
    "\n",
    "def preprocess_text(text):\n",
    "  cleaned = re.sub(r'\\W+', ' ', text).lower()\n",
    "  tokenized = word_tokenize(cleaned)\n",
    "  tokenized = [word for word in tokenized if word not in stop_words]\n",
    "  normalized = \" \".join([normalizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized if not re.match(r'\\d+',token)])\n",
    "  return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://insights.blackcoffer.com/how-is-login-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://insights.blackcoffer.com/how-does-ai-h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-its-im...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://insights.blackcoffer.com/how-do-deep-l...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://insights.blackcoffer.com/how-artificia...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL  POSITIVE SCORE  \\\n",
       "0       1  https://insights.blackcoffer.com/how-is-login-...             NaN   \n",
       "1       2  https://insights.blackcoffer.com/how-does-ai-h...             NaN   \n",
       "2       3  https://insights.blackcoffer.com/ai-and-its-im...             NaN   \n",
       "3       4  https://insights.blackcoffer.com/how-do-deep-l...             NaN   \n",
       "4       5  https://insights.blackcoffer.com/how-artificia...             NaN   \n",
       "\n",
       "   NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
       "0             NaN             NaN                 NaN                  NaN   \n",
       "1             NaN             NaN                 NaN                  NaN   \n",
       "2             NaN             NaN                 NaN                  NaN   \n",
       "3             NaN             NaN                 NaN                  NaN   \n",
       "4             NaN             NaN                 NaN                  NaN   \n",
       "\n",
       "   PERCENTAGE OF COMPLEX WORDS  FOG INDEX  AVG NUMBER OF WORDS PER SENTENCE  \\\n",
       "0                          NaN        NaN                               NaN   \n",
       "1                          NaN        NaN                               NaN   \n",
       "2                          NaN        NaN                               NaN   \n",
       "3                          NaN        NaN                               NaN   \n",
       "4                          NaN        NaN                               NaN   \n",
       "\n",
       "   COMPLEX WORD COUNT  WORD COUNT  SYLLABLE PER WORD  PERSONAL PRONOUNS  \\\n",
       "0                 NaN         NaN                NaN                NaN   \n",
       "1                 NaN         NaN                NaN                NaN   \n",
       "2                 NaN         NaN                NaN                NaN   \n",
       "3                 NaN         NaN                NaN                NaN   \n",
       "4                 NaN         NaN                NaN                NaN   \n",
       "\n",
       "   AVG WORD LENGTH  \n",
       "0              NaN  \n",
       "1              NaN  \n",
       "2              NaN  \n",
       "3              NaN  \n",
       "4              NaN  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Envy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6122922360218244\n"
     ]
    }
   ],
   "source": [
    "# from reviews import counter, training_counts\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import os\n",
    "from natsort import natsorted\n",
    "nltk.download('punkt')\n",
    "# from preprocessing import preprocess_text\n",
    "\n",
    "path = './train/'\n",
    "\n",
    "\n",
    "with open(path + 'positive.txt', 'r') as p:\n",
    "  pos = p.read()\n",
    "\n",
    "with open(path + 'negative.txt', 'r')as n:\n",
    "  neg = n.read()\n",
    "\n",
    "posi = preprocess_text(pos)\n",
    "positive = word_tokenize(posi)\n",
    "\n",
    "\n",
    "negi = preprocess_text(neg)\n",
    "negative = word_tokenize(negi)\n",
    "\n",
    "training = positive + negative\n",
    "\n",
    "\n",
    "# Saved all .txt file in a valriable called articles\n",
    "articls = [file for file in os.listdir() if file[-3:] == 'txt']\n",
    "articles = natsorted(articls)\n",
    "\n",
    "counter = CountVectorizer()\n",
    "\n",
    "training_counts = counter.fit_transform(training)\n",
    "\n",
    "# Add your review:\n",
    "# review = \"disappointed\"\n",
    "# review_counts = counter.transform([review])\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "\n",
    "training_labels = [1] * len(positive) + [0] * len(negative)\n",
    "\n",
    "classifier.fit(training_counts, training_labels)\n",
    "\n",
    "print(classifier.score(training_counts,training_labels))\n",
    "\n",
    "article_list = []\n",
    "for article in articles:\n",
    "  with open(article, 'r',encoding='utf-8') as f:\n",
    "    articlee = f.read()\n",
    "    article_list.append(articlee)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyse(articles):\n",
    "  global count\n",
    "  count_pos = 0\n",
    "  count_neg = 0  \n",
    "  cleaner = preprocess_text(articles)\n",
    "  cleaned = word_tokenize(cleaner)\n",
    "  print(cleaned)\n",
    "  for word in cleaned:\n",
    "    guess_vec = counter.transform([word])\n",
    "    neg = (classifier.predict_proba(guess_vec)[0][0] * 100).round()\n",
    "    pos = (classifier.predict_proba(guess_vec)[0][1] * 100).round()\n",
    "    if pos > 50:\n",
    "      count_pos += 1\n",
    "    elif neg > 50:\n",
    "       count_neg += 1\n",
    "    else:\n",
    "      pass\n",
    "  df['POSITIVE SCORE'].iloc[count]=count_pos\n",
    "  df['NEGATIVE SCORE'].iloc[count]=count_neg\n",
    "  polary = (count_pos - count_neg)/ ((count_pos + count_neg) + 0.000001)\n",
    "  df['POLARITY SCORE'].iloc[count]= polary\n",
    "  # Subjectivity = (count_pos + count_neg)/ ((cleaned) + 0.000001)\n",
    "  # df['SUBJECTIVITY SCORE'].iloc[count]= Subjectivity\n",
    "  count +=1\n",
    "  return count_pos, count_neg, count\n",
    "\n",
    "\n",
    "count = 0\n",
    "for i in range(len(article_list)):\n",
    "  print(analyse(article_list[i]))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://insights.blackcoffer.com/how-is-login-...</td>\n",
       "      <td>289.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>-0.026936</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://insights.blackcoffer.com/how-does-ai-h...</td>\n",
       "      <td>260.0</td>\n",
       "      <td>239.0</td>\n",
       "      <td>0.042084</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-its-im...</td>\n",
       "      <td>784.0</td>\n",
       "      <td>661.0</td>\n",
       "      <td>0.085121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://insights.blackcoffer.com/how-do-deep-l...</td>\n",
       "      <td>172.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://insights.blackcoffer.com/how-artificia...</td>\n",
       "      <td>742.0</td>\n",
       "      <td>751.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL  POSITIVE SCORE  \\\n",
       "0       1  https://insights.blackcoffer.com/how-is-login-...           289.0   \n",
       "1       2  https://insights.blackcoffer.com/how-does-ai-h...           260.0   \n",
       "2       3  https://insights.blackcoffer.com/ai-and-its-im...           784.0   \n",
       "3       4  https://insights.blackcoffer.com/how-do-deep-l...           172.0   \n",
       "4       5  https://insights.blackcoffer.com/how-artificia...           742.0   \n",
       "\n",
       "   NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
       "0           305.0       -0.026936                 NaN                  NaN   \n",
       "1           239.0        0.042084                 NaN                  NaN   \n",
       "2           661.0        0.085121                 NaN                  NaN   \n",
       "3           164.0        0.023810                 NaN                  NaN   \n",
       "4           751.0        0.000000                 NaN                  NaN   \n",
       "\n",
       "   PERCENTAGE OF COMPLEX WORDS  FOG INDEX  AVG NUMBER OF WORDS PER SENTENCE  \\\n",
       "0                          NaN        NaN                               NaN   \n",
       "1                          NaN        NaN                               NaN   \n",
       "2                          NaN        NaN                               NaN   \n",
       "3                          NaN        NaN                               NaN   \n",
       "4                          NaN        NaN                               NaN   \n",
       "\n",
       "   COMPLEX WORD COUNT  WORD COUNT  SYLLABLE PER WORD  PERSONAL PRONOUNS  \\\n",
       "0                 NaN         NaN                NaN                NaN   \n",
       "1                 NaN         NaN                NaN                NaN   \n",
       "2                 NaN         NaN                NaN                NaN   \n",
       "3                 NaN         NaN                NaN                NaN   \n",
       "4                 NaN         NaN                NaN                NaN   \n",
       "\n",
       "   AVG WORD LENGTH  \n",
       "0              NaN  \n",
       "1              NaN  \n",
       "2              NaN  \n",
       "3              NaN  \n",
       "4              NaN  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-06"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.around(0.000001, decimals = 6)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
